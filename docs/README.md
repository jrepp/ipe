# IPE Performance Test Results

This directory contains the interactive D3.js visualization for IPE predicate execution performance tests.

## 🌐 Live Dashboard

Once GitHub Pages is enabled, the performance dashboard will be available at:
```
https://[username].github.io/ipe/
```

## 📊 Files

- **index.html** - Interactive D3.js performance dashboard
- **perftest-results.json** - Performance test data (generated by running tests)

## 🚀 Setup GitHub Pages

To enable GitHub Pages for this repository:

1. Go to your repository on GitHub
2. Click **Settings** → **Pages** (in the left sidebar)
3. Under **Source**, select:
   - Branch: `main` (or your default branch)
   - Folder: `/docs`
4. Click **Save**
5. GitHub will build and deploy the site (takes 1-2 minutes)
6. The URL will be shown at the top of the Pages settings

## 📈 Updating Results

To update the performance dashboard with new test results:

```bash
# From project root
just perftest-all

# Or manually
cd crates/ipe-core
cargo run --release --bin perftest_runner --features jit
```

This will:
1. Run all performance tests (~3 minutes)
2. Generate `perftest-results.json` in both:
   - `crates/ipe-core/perftest-results.json` (local development)
   - `docs/perftest-results.json` (GitHub Pages)
3. Commit and push the updated `docs/perftest-results.json`

## 🎨 Dashboard Features

The interactive dashboard includes:

- **6 Visualization Types:**
  1. Latency comparison bar chart
  2. Throughput comparison
  3. Latency distribution box plots
  4. JIT cache hit rate pie chart
  5. Performance heatmap
  6. JIT vs Interpreter speedup comparison

- **Interactive Features:**
  - Filter by executor (interpreter/JIT)
  - Filter by workload type
  - Switch metrics (p50/p95/p99/throughput)
  - Hover tooltips with detailed statistics
  - Export data as JSON

- **Statistical Analysis:**
  - Comprehensive outlier detection
  - Percentile latencies (p50, p95, p99)
  - Throughput measurements
  - JIT cache hit rates

## 📝 Local Development

To view the dashboard locally:

```bash
# After running perftests
cd docs
python3 -m http.server 8000

# Open in browser
open http://localhost:8000
```

Or simply open `docs/index.html` in your browser (may have CORS issues with file://).

## 🔄 CI/CD Integration

The `docs/perftest-results.json` file should be updated and committed after running performance tests. Consider:

1. Running tests on a schedule (weekly/monthly)
2. Committing updated results to track performance over time
3. Comparing historical results to detect regressions

## 📖 More Information

- **Quick Start:** See `/PERFTEST_QUICKSTART.md`
- **Full Documentation:** See `/crates/ipe-core/PERFTEST.md`
- **Usage Guide:** See `/crates/ipe-core/README_PERFTEST.md`
